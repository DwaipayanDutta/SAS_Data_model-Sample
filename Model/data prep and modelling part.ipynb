{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('path to csv file')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marital Status\n",
    "df['MARITAL_STATUS'] = df['MARITAL_STATUS'].str.strip().str.upper()\n",
    "df['MARITAL_STATUS'] = np.where(df['MARITAL_STATUS'].isin(['WIDOW', 'DIVORCED', 'WIDOWER', 'LEGALLY SEPARATED']), 'WIDOWED/SEPARATED/DIVORCED', \n",
    "                                                          np.where(df['MARITAL_STATUS'].isin(['UNMARRIED', 'LIVE-IN RELATIONSHIP']), 'UNMARRIED/ SINGLE',\n",
    "                                                                                             np.where(df['MARITAL_STATUS'].isin(['UNMARRIED', 'LIVE-IN RELATIONSHIP']), 'UNMARRIED/ SINGLE', df['MARITAL_STATUS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occupation\n",
    "df['OCCUPATION'] = df['OCCUPATION'].str.strip().str.upper()\n",
    "df['OCCUPATION'] = np.where(df['OCCUPATION'].isin(['OTHERS','CURRENCY','TRANSACTION','INFORMATION TECHNOLOGY','SENP - OTHERS','CURRENT_EMPLOYMENT','FINANCE','SENP - MANUFACTURER','AGENCY','NON WORKING','TRADERS - SHARE / COMMODITY','SENP - SERVICE INDUSTRY','EVENT MANAGEMENT','BULLION / GEM / JEWEL / PRECIOUS METAL TRADER']), 'OTHERS', \n",
    "                            np.where(df['OCCUPATION'].isin(['BUSINESS','SMALL BUSINESSMAN','INDUSTRIALIST','DOCTOR','ART AND ANTIQUE DEALER','HARDWARE']), 'BUSINNESS OWNER', \n",
    "                                     np.where(df['OCCUPATION'].isin(['SALARIED','PVT EMPLOYEE','PROFESSIONAL','PUBLIC UTILITIES AND SERVICES','ENGINEER','ARCHITECT','MNC','DEFENCE FORCE']), 'SALARIED',\n",
    "                                              np.where(df['OCCUPATION'].isin(['SELF EMPLOYED','SELF EMPLOYEES PROFESSIONAL (SEP)','AGRICULTURE AND ALLIED ACTIVITIES','TRADER','DOCTOR/ DENTIST','GOVT SERVICE (SELF EMPLOYED)','LAWYER','REAL ESTATE BROKERS','ENGINEERING CONSULTANT','TRANSPORT OPERATOR','CHARTERED ACCOUNTANT','CONSULTANT','AGRICULTURE','Self employed','CA-CS','BROKERS','GOVT FREE LANCER','SHOPKEEPER','FARMER']), 'SELF EMPLOYED', df['OCCUPATION']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dic = {'STATE': {0: 'MAH',\n",
    "  1: 'DLI',\n",
    "  2: 'UTT',\n",
    "  3: 'GUJ',\n",
    "  4: 'HARY',\n",
    "  5: 'PUN',\n",
    "  6: 'WES',\n",
    "  7: 'RAJ',\n",
    "  8: 'TMU',\n",
    "  9: 'KRA',\n",
    "  10: 'ADH',\n",
    "  11: 'MDH',\n",
    "  12: 'BIH',\n",
    "  13: 'KER',\n",
    "  14: 'ORI',\n",
    "  15: 'JHA',\n",
    "  16: 'CHHA',\n",
    "  17: 'ASS',\n",
    "  18: 'TEA',\n",
    "  19: 'CA',\n",
    "  20: 'UTK',\n",
    "  21: 'HIM',\n",
    "  22: 'TLG',\n",
    "  23: 'JMR',\n",
    "  24: 'GOA',\n",
    "  25: 'CA)',\n",
    "  26: '.',\n",
    "  27: 'TRI',\n",
    "  28: 'SIK',\n",
    "  29: 'MNR',\n",
    "  30: 'PON',\n",
    "  31: 'NAG',\n",
    "  32: 'MEG',\n",
    "  33: 'DAD',\n",
    "  34: 'AUH',\n",
    "  35: 'MIZ',\n",
    "  36: 'DAM',\n",
    "  37: 'DUB',\n",
    "  38: 'ODISH',\n",
    "  39: 'US',\n",
    "  40: 'UTL',\n",
    "  41: 'AE',\n",
    "  42: 'JAR',\n",
    "  43: 'SG',\n",
    "  44: 'GB',\n",
    "  45: 'CHATT',\n",
    "  46: 'ABU',\n",
    "  47: 'ADR',\n",
    "  48: 'JP',\n",
    "  49: 'FRA',\n",
    "  50: 'CN',\n",
    "  51: 'LK',\n",
    "  52: 'SWE',\n",
    "  53: 'NG',\n",
    "  54: 'CDA',\n",
    "  55: 'ASA',\n",
    "  56: 'CAN',\n",
    "  57: 'UTTRA',\n",
    "  58: 'KR',\n",
    "  59: 'SA',\n",
    "  60: 'DHA',\n",
    "  61: 'AND',\n",
    "  62: 'MARY',\n",
    "  63: 'TEL',\n",
    "  64: 'NET',\n",
    "  65: 'NZ',\n",
    "  66: 'MUSC',\n",
    "  67: 'FOA',\n",
    "  68: 'LANTA',\n",
    "  69: 'KW',\n",
    "  70: 'SWI',\n",
    "  71: 'NWY',\n",
    "  72: 'DE',\n",
    "  73: 'BEL',\n",
    "  74: 'TEX',\n",
    "  75: 'LON',\n",
    "  76: 'CZ',\n",
    "  77: 'SHA',\n",
    "  78: 'KE',\n",
    "  79: 'GOI',\n",
    "  80: 'IT',\n",
    "  81: 'ONT',\n",
    "  82: 'BRN',\n",
    "  83: 'RSA',\n",
    "  84: 'KTU',\n",
    "  85: 'PUE',\n",
    "  86: 'KAM',\n",
    "  87: 'PENN',\n",
    "  88: 'DEL',\n",
    "  89: 'MY',\n",
    "  90: 'IND',\n",
    "  91: 'BNK',\n",
    "  92: 'DEC',\n",
    "  93: 'FR',\n",
    "  94: 'HAR',\n",
    "  95: 'TMS',\n",
    "  96: 'TOK',\n",
    "  97: 'NWK',\n",
    "  98: 'HK',\n",
    "  99: 'QAT',\n",
    "  100: 'BHN',\n",
    "  101: 'SAH',\n",
    "  102: 'KEN',\n",
    "  103: 'GEO',\n",
    "  104: 'NP',\n",
    "  105: 'NL',\n",
    "  106: 'HAMPS',\n",
    "  107: 'HAW',\n",
    "  108: 'PAT',\n",
    "  109: 'SING',\n",
    "  110: 'TLK',\n",
    "  111: 'MHM'},\n",
    " 'STATE_CAT': {0: 'MAHARASHTRA',\n",
    "  1: 'NRI OR OTHERS',\n",
    "  2: 'UTTAR PRADESH',\n",
    "  3: 'GUJARAT',\n",
    "  4: 'HARYANA',\n",
    "  5: 'PUNJAB',\n",
    "  6: 'WEST BENGAL',\n",
    "  7: 'RAJASTHAN',\n",
    "  8: 'TAMIL NADU',\n",
    "  9: 'KARNATAKA',\n",
    "  10: 'ANDHRA PRADESH',\n",
    "  11: 'MADHYA PRADESH',\n",
    "  12: 'BIHAR',\n",
    "  13: 'KERALA',\n",
    "  14: 'ORISSA',\n",
    "  15: 'JHARKHAND',\n",
    "  16: 'CHHATTISGARH',\n",
    "  17: 'ASSAM',\n",
    "  18: 'NRI OR OTHERS',\n",
    "  19: 'NRI OR OTHERS',\n",
    "  20: 'UTTARAKHAND',\n",
    "  21: 'HIMACHAL PRADESH',\n",
    "  22: 'NRI OR OTHERS',\n",
    "  23: 'JAMMU & KASHMIR',\n",
    "  24: 'GOA',\n",
    "  25: 'NRI OR OTHERS',\n",
    "  26: 'NRI OR OTHERS',\n",
    "  27: 'TIRIPURA',\n",
    "  28: 'SIKKIM',\n",
    "  29: 'MANIPUR',\n",
    "  30: 'PONDICHERRY',\n",
    "  31: 'NAGALAND',\n",
    "  32: 'MEGALAYA',\n",
    "  33: 'DADRA & NAGAR HAVELI',\n",
    "  34: 'NRI OR OTHERS',\n",
    "  35: 'MIZORAM',\n",
    "  36: 'DAMAN AND DIU',\n",
    "  37: 'NRI OR OTHERS',\n",
    "  38: 'ORISSA',\n",
    "  39: 'NRI OR OTHERS',\n",
    "  40: 'NRI OR OTHERS',\n",
    "  41: 'NRI OR OTHERS',\n",
    "  42: 'NRI OR OTHERS',\n",
    "  43: 'NRI OR OTHERS',\n",
    "  44: 'NRI OR OTHERS',\n",
    "  45: 'CHHATTISGARH',\n",
    "  46: 'NRI OR OTHERS',\n",
    "  47: 'NRI OR OTHERS',\n",
    "  48: 'NRI OR OTHERS',\n",
    "  49: 'NRI OR OTHERS',\n",
    "  50: 'NRI OR OTHERS',\n",
    "  51: 'NRI OR OTHERS',\n",
    "  52: 'NRI OR OTHERS',\n",
    "  53: 'NRI OR OTHERS',\n",
    "  54: 'NRI OR OTHERS',\n",
    "  55: 'NRI OR OTHERS',\n",
    "  56: 'NRI OR OTHERS',\n",
    "  57: 'UTTARANCHAL',\n",
    "  58: 'NRI OR OTHERS',\n",
    "  59: 'NRI OR OTHERS',\n",
    "  60: 'NRI OR OTHERS',\n",
    "  61: 'ANDAMAN AND NICOBAR',\n",
    "  62: 'NRI OR OTHERS',\n",
    "  63: 'TELANGANA',\n",
    "  64: 'NRI OR OTHERS',\n",
    "  65: 'NRI OR OTHERS',\n",
    "  66: 'NRI OR OTHERS',\n",
    "  67: 'NRI OR OTHERS',\n",
    "  68: 'NRI OR OTHERS',\n",
    "  69: 'NRI OR OTHERS',\n",
    "  70: 'NRI OR OTHERS',\n",
    "  71: 'NRI OR OTHERS',\n",
    "  72: 'NRI OR OTHERS',\n",
    "  73: 'NRI OR OTHERS',\n",
    "  74: 'NRI OR OTHERS',\n",
    "  75: 'NRI OR OTHERS',\n",
    "  76: 'NRI OR OTHERS',\n",
    "  77: 'NRI OR OTHERS',\n",
    "  78: 'NRI OR OTHERS',\n",
    "  79: 'NRI OR OTHERS',\n",
    "  80: 'NRI OR OTHERS',\n",
    "  81: 'NRI OR OTHERS',\n",
    "  82: 'NRI OR OTHERS',\n",
    "  83: 'NRI OR OTHERS',\n",
    "  84: 'NRI OR OTHERS',\n",
    "  85: 'NRI OR OTHERS',\n",
    "  86: 'NRI OR OTHERS',\n",
    "  87: 'NRI OR OTHERS',\n",
    "  88: 'DELHI',\n",
    "  89: 'NRI OR OTHERS',\n",
    "  90: 'NRI OR OTHERS',\n",
    "  91: 'NRI OR OTHERS',\n",
    "  92: 'NRI OR OTHERS',\n",
    "  93: 'NRI OR OTHERS',\n",
    "  94: 'NRI OR OTHERS',\n",
    "  95: 'NRI OR OTHERS',\n",
    "  96: 'NRI OR OTHERS',\n",
    "  97: 'NRI OR OTHERS',\n",
    "  98: 'NRI OR OTHERS',\n",
    "  99: 'NRI OR OTHERS',\n",
    "  100: 'NRI OR OTHERS',\n",
    "  101: 'NRI OR OTHERS',\n",
    "  102: 'NRI OR OTHERS',\n",
    "  103: 'NRI OR OTHERS',\n",
    "  104: 'NRI OR OTHERS',\n",
    "  105: 'NRI OR OTHERS',\n",
    "  106: 'NRI OR OTHERS',\n",
    "  107: 'NRI OR OTHERS',\n",
    "  108: 'NRI OR OTHERS',\n",
    "  109: 'NRI OR OTHERS',\n",
    "  110: 'NRI OR OTHERS',\n",
    "  111: 'NRI OR OTHERS'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_map = pd.DataFrame(state_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.merge(df, state_map, on = 'STATE', how = 'left') # This line needs to be uncommented before sharing it with IBL\n",
    "df['STATE_CAT'] = df['STATE'].str.strip().str.upper() # this line needs to be commented before sharing with IBL\n",
    "\n",
    "north = ['DELHI','HARYANA','UTTAR PRADESH','CHANDIGARH','RAJASTHAN','PUNJAB','UTTARANCHAL','JAMMU & KASHMIR','HIMACHAL PRADESH','UTTARAKHAND','MADHYA PRADESH']\n",
    "east = ['WEST BENGAL','CHHATTISGARH','JHARKHAND','ORISSA','ASSAM','BIHAR','TRIPURA','SIKKIM','MEGHALAYA','MANIPUR','ARUNACHAL PRADESH','MIZORAM','NAGALAND']\n",
    "west = ['RAJASTHAN','MAHARASHTRA','GOA','GUJARAT','MADHYA PRADESH','DAMAN AND DIU','DADRA & NAGAR HAVELI']\n",
    "south = ['ANDHRA PRADESH','KARNATAKA','TAMIL NADU','TELANGANA','KERALA','PONDICHERRY']\n",
    "\n",
    "df['ZONE'] = np.where(df['STATE_CAT'].isin(north), 'NORTH', \n",
    "                      np.where(df['STATE_CAT'].isin(east), 'EAST',\n",
    "                               np.where(df['STATE_CAT'].isin(west), 'WEST',\n",
    "                                        np.where(df['STATE_CAT'].isin(south), 'SOUTH', 'NRI/ OUT OF INDIA'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['KYC_LAST_DONE_DATE'] = pd.to_datetime(df['KYC_LAST_DONE_DATE'])\n",
    "df = df[~df['KYC_LAST_DONE_DATE'].isna()].reset_index(drop = True)\n",
    "df['KYC_RECENCY_YEARS'] = (datetime.now() - df['KYC_LAST_DONE_DATE'])/np.timedelta64(1, 'D')/365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create buckets\n",
    "def assign_bucket(var, ranges, label):\n",
    "    for i, range in enumerate(ranges):\n",
    "        if var > range:\n",
    "            return label[i]\n",
    "    return label[-1] if var > range[-1] else 'Not Available'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyc_ranges = [0.5, 1, 3, 5]\n",
    "kyc_labels = ['Less than 6 Months', '6 Months to 1 Year', '1 to 3 Years', '3 to 5 Years', 'More than 5 Years']\n",
    "\n",
    "df['KYC_RECENCY_BUCKET'] = df['KYC_RECENCY_YEARS'].apply(assign_bucket, args = (kyc_ranges,kyc_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting target into binary\n",
    "df['TARGET_LI'] = np.where(df['TARGET_LI']=='Y', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['ACCOUNT_TYPE',\n",
    "'CUSTOMER_TAG', # Has lot of values - need input from the bank\n",
    "'EDUCATION_LEVEL',\n",
    "'GENDER',\n",
    "'INTERNET_BANKING_USAGE',\n",
    "'MARITAL_STATUS',\n",
    "'NOMINEE_AVAILABLE_FLAG',\n",
    "'RM_ALLOCATED_FLAG',\n",
    "'OCCUPATION',\n",
    "# 'STATE',\n",
    "'KYC_RECENCY_BUCKET', # Derived\n",
    "'FD_FLAG', \n",
    "'GI_FLAG', \n",
    "'HEALTH_FLAG', \n",
    "'MASS_FLAG', \n",
    "'MF_FLAG', \n",
    "'NR_FLAG',\n",
    "'ZONE' # Derived\n",
    "]\n",
    "\n",
    "num_cols = ['AGE', 'AQB_BALANCE', 'AUM', 'CIBIL_SCORE', 'CREDIT_CARD_LIMIT', 'CR_AMT_12MNTH', 'CR_CNT_12MNTH',\n",
    "            'DC_APPAREL_30DAYS_ACTV', 'DC_ECOM_30DAYS_AMT', 'DC_ECOM_30DAYS_CNT', 'DC_FOOD_30DAYS_ACTV',\n",
    "            'DC_FUEL_30DAYS_ACTV', 'DC_GROCRY_30DAYS_ACTV', 'DC_OTT_30DAYS_ACTV', 'DC_POS_30DAYS_AMT',\n",
    "            'DC_POS_30DAYS_CNT', 'DC_RECHARGE_30DAYS_ACTV', 'DC_TRAVEL_30DAYS_ACTV', 'DC_UTILITY_30DAYS_ACTV',\n",
    "            'DR_AMT_12MNTH', 'DR_CNT_12MNTH', 'DR_CR_RATIO', 'FD_COUNT', 'FD_CURRENTMONTHANR',  # 'INCOME_NET', removing due to less fill rate\n",
    "            'MONTHLY_BALANCE', 'NRV', 'TOTAL_LIVE_SECURED_AMT', 'TOTAL_LIVE_UNSECURED_AMT',\n",
    "            'VINTAGE_DAYS']\n",
    "\n",
    "target = 'TARGET_LI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value treatment for categorical variables\n",
    "for cat in cat_cols:\n",
    "    df[cat] = np.where(df[cat].isna(), 'Not Available', df[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_valid = train_test_split(df, test_size=0.4, random_state=42, stratify=df['TARGET_LI'])\n",
    "test, valid = train_test_split(test_valid, test_size=0.5, random_state=42, stratify=test_valid['TARGET_LI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET_LI\n",
       "0    0.850677\n",
       "1    0.149323\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TARGET_LI'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET_LI\n",
       "0    0.850668\n",
       "1    0.149332\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['TARGET_LI'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET_LI\n",
       "0    0.850682\n",
       "1    0.149318\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid['TARGET_LI'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value treatment for continuous variables\n",
    "# We are assuming that all the continious variables are skewed and median will be the better representative of the central tendency\n",
    "# Hence we will replace any missing value with respective median value\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputer_fit = imputer.fit(train[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[num_cols] = imputer_fit.fit_transform(train[num_cols])\n",
    "test[num_cols] = imputer_fit.fit_transform(test[num_cols])\n",
    "valid[num_cols] = imputer_fit.fit_transform(valid[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling numerical parameters using standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaler_fit = scaler.fit(train[num_cols])\n",
    "\n",
    "train[num_cols] = scaler_fit.fit_transform(train[num_cols])\n",
    "test[num_cols] = scaler_fit.fit_transform(test[num_cols])\n",
    "valid[num_cols] = scaler_fit.fit_transform(valid[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding categorical variables\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "for col in cat_cols:\n",
    "    train[col] = le.fit_transform(train[col])\n",
    "    test[col] = le.fit_transform(test[col])\n",
    "    valid[col] = le.fit_transform(valid[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Model Visualization script\n",
    "class ModelViz:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_optimum_threshold(df, target='Target', score='Score'):\n",
    "        '''\n",
    "        Given probability scores and binary target, returns the optimum cut off point, where \n",
    "        `true positive rate` is high and `false positive rate` is low.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas.DataFrame, Dataframe that contains binary target values - 0 & 1 and prediction scores\n",
    "        target: str, Name of the target column. Default = Target\n",
    "        score: str, Name of the probability score column. Default = Score\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: returns ROC AUC\n",
    "        pd.DataFrame: returns a new DataFrame that provides tpr, fpr and optimum threshold\n",
    "        matplotlib.pyplot: returns a ROC curve with cut-off point\n",
    "        matplotlib.pyplot: returns a Target Separability Plot with threshold\n",
    "\n",
    "        '''\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(df[target], df['Score'])\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        ####################################\n",
    "        # The optimal cut off would be where tpr is high and fpr is low\n",
    "        # tpr - (1-fpr) is zero or near to zero is the optimal cut off point\n",
    "        ####################################\n",
    "        i = np.arange(len(tpr))  # index for df\n",
    "        roc = pd.DataFrame({\n",
    "            'fpr': pd.Series(fpr, index=i),\n",
    "            'tpr': pd.Series(tpr, index=i),\n",
    "            '1-fpr': pd.Series(1-fpr, index=i),\n",
    "            'tf': pd.Series(tpr - (1-fpr), index=i),\n",
    "            'thresholds': pd.Series(thresholds, index=i)\n",
    "        })\n",
    "\n",
    "        cutoff_df = roc.iloc[(roc.tf-0).abs().argsort()\n",
    "                             [:1]].reset_index(drop=True)\n",
    "\n",
    "        # Plot tpr vs 1-fpr\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(roc['tpr'], label='tpr')\n",
    "        plt.plot(roc['1-fpr'], color='red', label='1-fpr')\n",
    "        plt.xlabel('1-False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        ax.set_xticklabels([])\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot tpr vs 1-fpr\n",
    "        fig2, ax2 = plt.subplots()\n",
    "        sns.kdeplot(x=df[df[target] == 0]['Score'], label='0')\n",
    "        sns.kdeplot(x=df[df[target] == 1]['Score'], label='1')\n",
    "        plt.axvline(x=cutoff_df['thresholds'].values[0],\n",
    "                    label='thresh={:.2f}'.format(cutoff_df['thresholds'].values[0]), color='red', ls='--')\n",
    "        plt.title('Target Separability')\n",
    "        plt.legend()\n",
    "\n",
    "        return roc_auc, cutoff_df, ax, ax2\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_classification_report(clf, X, y, thres=0.5):\n",
    "        '''\n",
    "        Given model, X and y provides classification report for the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        clf: sklearn model, trained classification model that has predict_proba available\n",
    "        X: pandas.DataFrame or numpy array, Dataframe/array that acts as independent variables for the model\n",
    "        y: pandas.Series or numpy 1D-array, Series/1D-array that acts as the dependant/target variable for the model\n",
    "        thres: float, optional. The probability threshold to determine 0 or 1. Default is 0.5\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        classification report: str, returns classification report\n",
    "\n",
    "        '''\n",
    "\n",
    "        x_train_proba = clf.predict_proba(X)[:, 1]\n",
    "        x_train_pred = np.where(x_train_proba > thres, 1, 0)\n",
    "\n",
    "        clf_report = metrics.classification_report(y, x_train_pred)\n",
    "\n",
    "        return clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decile summary generation function\n",
    "\n",
    "def decile_fun(score, prob_list):\n",
    "  if score >= prob_list[0]:\n",
    "    decile = 1\n",
    "  elif score >= prob_list[1] < prob_list[0]:\n",
    "    decile = 2\n",
    "  elif score >= prob_list[2] < prob_list[1]:\n",
    "    decile = 3\n",
    "  elif score >= prob_list[3] < prob_list[2]:\n",
    "    decile = 4\n",
    "  elif score >= prob_list[4] < prob_list[3]:\n",
    "    decile = 5\n",
    "  elif score >= prob_list[5] < prob_list[4]:\n",
    "    decile = 6\n",
    "  elif score >= prob_list[6] < prob_list[5]:\n",
    "    decile = 7\n",
    "  elif score >= prob_list[7] < prob_list[6]:\n",
    "    decile = 8\n",
    "  elif score >= prob_list[8] < prob_list[7]:\n",
    "    decile = 9\n",
    "  else:\n",
    "    decile = 10\n",
    "  return decile\n",
    "\n",
    "def decile_summary(prob, actual, prob_list, req_dig=True):\n",
    "  Decile = [decile_fun(col, prob_list) for col in prob]\n",
    "  results = pd.DataFrame({'Sum': actual,'Count': actual,'Probability': prob}).reset_index(drop = True)\n",
    "  results['Decile'] = Decile\n",
    "  decile_sum = results.groupby('Decile')['Sum'].sum().reset_index()\n",
    "  decile_cumsum = decile_sum['Sum'].cumsum().reset_index()\n",
    "  decile_cumsum.columns = ['Decile', 'CumSum']\n",
    "  decile_cumsum['Decile'] = decile_cumsum['Decile'] + 1\n",
    "  decile_count = results.groupby('Decile')['Count'].count().reset_index()\n",
    "  Decile_sum=decile_sum.join(decile_count.set_index('Decile'),on='Decile')\n",
    "  Decile_sum=Decile_sum.join(decile_cumsum.set_index('Decile'),on='Decile')\n",
    "  Decile_sum['gain'] = Decile_sum['CumSum']/decile_sum['Sum'].sum()\n",
    "  Decile_sum['Event Rate']=Decile_sum['Sum']/Decile_sum['Count']\n",
    "  if req_dig:\n",
    "    ax = plt.figure(figsize=(12, 8))\n",
    "    plt.title('Decile Score - Cumulative Gain Plot')\n",
    "    sns.lineplot(\n",
    "        x = Decile_sum['Decile'], y=Decile_sum['gain']*100, label='model')\n",
    "    sns.lineplot(x=Decile_sum['Decile'],\n",
    "                  y=Decile_sum['Decile']*10, label='avg')\n",
    "\n",
    "    return Decile, Decile_sum, ax\n",
    "  else:\n",
    "      return Decile, Decile_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00253614986770205 -0.0004451814283855314\n",
      "0.008575486946080438 -0.0015052925603396446\n",
      "0.008295995475034304 -0.001456232205552699\n",
      "0.0063064545379703 -0.001106999422635326\n",
      "0.0024146978715706975 -0.0004238624307166435\n",
      "0.0002160552959822527 -3.79251267840059e-05\n",
      "-0.0006367795528821889 0.00011177668738323908\n",
      "-0.0020760434843915786 0.0003644169516721535\n",
      "0.003247027921665204 -0.0005699649482799186\n",
      "0.00916616651316735 -0.0016089771165019372\n",
      "0.003265353274583539 -0.0005731816772642155\n",
      "0.0006833441959738146 -0.00011995038192212307\n",
      "-0.00042632330897633913 7.483438658790021e-05\n",
      "-0.003106255926108054 0.0005452546269946276\n",
      "-0.0015349259728142846 0.00026943223890101405\n",
      "-0.002437263210752356 0.0004278234229498583\n",
      "0.000639861250266786 -0.00011231763114225446\n",
      "-0.009174013530943869 0.0016103545376972524\n",
      "0.007337436863065579 -0.0012879722389388748\n",
      "-0.0006651711065190702 0.00011676038040663279\n",
      "0.004568065539979533 -0.0008018524330700584\n",
      "-0.0026182053249601566 0.0004595849800582441\n",
      "0.00534547792962543 -0.000938315014589653\n",
      "-0.0011563566702803443 0.00020298032097978869\n",
      "-0.00032116766147757754 5.637595794678125e-05\n",
      "-0.006368117368027866 0.0011178233676681529\n",
      "0.009030785631912123 -0.0015852131209821766\n",
      "0.008243304128207105 -0.0014469830640322305\n",
      "0.009355749748615314 -0.0016422554872439503\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Chi-Square Test and Cramer's v\n",
    "def chi_sq_test(df, x,y):\n",
    "    cross_tabs = pd.crosstab(df[x], df[y])\n",
    "    chi2, p, dof, con_table = stats.chi2_contingency(cross_tabs)\n",
    "    if p < 0.05:\n",
    "        decision = 'Reject H0: there is significant association between ' + x + ' and ' + y\n",
    "        # calculating cramer's v\n",
    "        n = cross_tabs.sum().sum()\n",
    "        minimum_dimension = min(cross_tabs.shape)-1\n",
    "        v = np.sqrt(chi2/(n*dof))\n",
    "        if v <= 0.2:\n",
    "            strength = 'Weak Association between '  + x + ' and ' + y\n",
    "        elif v > 0.2 and v <= 0.6:\n",
    "            strength = 'Medium Association between '  + x + ' and ' + y\n",
    "        else:\n",
    "            strength = 'Strong Association between '  + x + ' and ' + y\n",
    "    else: \n",
    "        decision = 'Do not reject H0: There is no relation between ' + x + ' and ' + y\n",
    "        strength = 'No association between '  + x + ' and ' + y\n",
    "        v = 0\n",
    "    print(f'chi-squared = {chi2}\\np value= {p}\\ndegrees of freedom = {dof}')\n",
    "    print(decision)\n",
    "    print(\"Cramer's V: \" + str(v))\n",
    "    print(strength)\n",
    "    return p\n",
    "\n",
    "\n",
    "def cont_test(df, x, target):\n",
    "    # Perform the two sample t-test with equal variances\n",
    "    t = stats.ttest_ind(a=df[df[target]==1][x], b=df[df[target]==0][x], equal_var=True)\n",
    "    print(df[df[target]==1][x].mean(), df[df[target]==0][x].mean())\n",
    "    return t.pvalue\n",
    "\n",
    "# cont Features selections\n",
    "Row = 0\n",
    "for i in num_cols:\n",
    "    out = cont_test(train, i, target)\n",
    "    if out < 0.05:\n",
    "        Row = Row + 1\n",
    "        if Row == 1:\n",
    "            significant = [i]\n",
    "        else:\n",
    "            significant = significant + [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat Features selections\n",
    "Row = 0\n",
    "for i in cat_cols:\n",
    "    out = chi_sq_test(train, i, 'TARGET_LI')\n",
    "    if out < 0.05:\n",
    "        Row = Row + 1\n",
    "        if Row == 1:\n",
    "            significant_cat = [i]\n",
    "        else:\n",
    "            significant_cat = significant_cat + [i]\n",
    "\n",
    "s_features = significant_cat + significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = s_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == 'RF':\n",
    "\n",
    "    # Random Forest\n",
    "    model = RandomForestClassifier(bootstrap=False,\n",
    "    max_depth= 4,\n",
    "    max_features= 'sqrt',\n",
    "    min_samples_leaf=55,\n",
    "    min_samples_split= 10,\n",
    "    n_estimators= 200, \n",
    "    class_weight='balanced', \n",
    "    random_state=42)\n",
    "\n",
    "    model.fit(train[features],train[target]) # cat_cols + num_cols needs to be replaced with s_features\n",
    "    importance = model.feature_importances_\n",
    "\n",
    "    # summarize feature importance\n",
    "    feature_imp = pd.DataFrame({'Feature':s_features, 'Importance':importance}).sort_values('Importance', ascending=False)\n",
    "    print('Feature Importance:\\n', feature_imp)\n",
    "else: \n",
    "    model = LogisticRegression(class_weight='balanced')\n",
    "    model.fit(train[features],train[target]) # cat_cols + num_cols needs to be replaced with s_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(test_df, features, target, iftrain = 'Yes'):\n",
    "    train_result = pd.DataFrame(test_df[target].copy()) \n",
    "    y_pred_train =[x[1] for x in model.predict_proba(test_df[features])] # feature needs to be replaced with s_feature\n",
    "    train_result['Score'] = y_pred_train\n",
    "    roc_auc, cutoff_df, ax, ax2 = ModelViz.get_optimum_threshold(train_result, target=target)\n",
    "    print(f'ROC AUC Score at an Optimum Threshold: {cutoff_df} \\n',roc_auc_score(test_df[target],y_pred_train))\n",
    "    if iftrain == 'Yes':\n",
    "        train_result['Decile'] = 10 - pd.qcut(train_result['Score'], 10, labels=False)\n",
    "        decile_prob = list(train_result.groupby('Decile')['Score'].min())\n",
    "        return train_result, decile_prob\n",
    "    else:\n",
    "        return train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result, decile_prob = model_result(train, features, target)\n",
    "training_decile, train_decile_summary, ax = decile_summary(prob = train_result['Score'], actual = train_result['TARGET_LI'], prob_list = decile_prob)\n",
    "print(train_decile_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = model_result(test, feature, target, iftrain=False)\n",
    "test_decile, test_decile_summary, ax = decile_summary(prob = test_result['Score'], actual = test_result['TARGET_LI'], prob_list = decile_prob)\n",
    "print(test_decile_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_result = model_result(valid, feature, target, iftrain=False)\n",
    "valid_decile, valid_decile_summary, ax = decile_summary(prob = valid_result['Score'], actual = valid_result['TARGET_LI'], prob_list = decile_prob)\n",
    "print(valid_decile_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
